{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPIS NOTEBOOK I PLAN DZIALANIA, moze cos o modelach klasycznych i dlaczego takie wybralem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plan działania:\n",
    "\n",
    "1. Wczytanie i wstępna analiza danych: Sprawdzenie typów danych, brakujących wartości, oraz podstawowych statystyk.\n",
    "\n",
    "2. Analiza korelacji cech: To pomoże nam zdecydować, które zmienne mogą być przydatne do predykcji produkcji energii.\n",
    "\n",
    "3. Wybór modeli i cech: Na podstawie analizy korelacji wybierzemy odpowiednie cechy do modelowania.\n",
    "\n",
    "4. Porównanie modeli na ustawieniach domyślnych: Trenowanie różnych modeli z biblioteki sklearn na danych i porównanie ich wyników.\n",
    "\n",
    "5. Optymalizacja hiperparametrów: Przetestowanie różnych ustawień hiperparametrów dla wybranych modeli.\n",
    "\n",
    "6. Normalizacja danych: Sprawdzenie, jak normalizacja wpływa na wyniki modeli.\n",
    "\n",
    "7. Ekstrapolacja w przyszłość: Użycie najlepszego modelu do predykcji na lata 2024-2035.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Wczytanie i wstępna analiza danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Wczytanie danych z pliku CSV\u001b[39;00m\n\u001b[0;32m      4\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../2. Data/processed/final_df_processed.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Wczytanie danych z pliku CSV\n",
    "data_path = '../2. Data/processed/final_df_processed.csv'\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>city_name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>temp</th>\n",
       "      <th>feels_like</th>\n",
       "      <th>temp_min</th>\n",
       "      <th>temp_max</th>\n",
       "      <th>pressure</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>clouds_all</th>\n",
       "      <th>weather_main</th>\n",
       "      <th>estimated_irradiance</th>\n",
       "      <th>temp_modifier</th>\n",
       "      <th>humidity_modifier</th>\n",
       "      <th>daily_energy_production_kWh</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1979-01-01</td>\n",
       "      <td>Baden_Baden</td>\n",
       "      <td>48.765640</td>\n",
       "      <td>8.228524</td>\n",
       "      <td>-10.442083</td>\n",
       "      <td>-16.532917</td>\n",
       "      <td>-16.18</td>\n",
       "      <td>3.02</td>\n",
       "      <td>1013.666667</td>\n",
       "      <td>81.916667</td>\n",
       "      <td>3.681667</td>\n",
       "      <td>100</td>\n",
       "      <td>Snow</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>5.7</td>\n",
       "      <td>1979</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1979-01-01</td>\n",
       "      <td>Bozanska</td>\n",
       "      <td>54.473028</td>\n",
       "      <td>18.302179</td>\n",
       "      <td>-14.100000</td>\n",
       "      <td>-20.617500</td>\n",
       "      <td>-19.14</td>\n",
       "      <td>-9.46</td>\n",
       "      <td>1002.416667</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>6.157917</td>\n",
       "      <td>100</td>\n",
       "      <td>Snow</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>5.7</td>\n",
       "      <td>1979</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1979-01-01</td>\n",
       "      <td>Walcz_Drugi</td>\n",
       "      <td>53.257039</td>\n",
       "      <td>16.521203</td>\n",
       "      <td>-16.437083</td>\n",
       "      <td>-23.322500</td>\n",
       "      <td>-21.26</td>\n",
       "      <td>-9.57</td>\n",
       "      <td>1003.708333</td>\n",
       "      <td>82.416667</td>\n",
       "      <td>3.682083</td>\n",
       "      <td>100</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>7.6</td>\n",
       "      <td>1979</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1979-01-02</td>\n",
       "      <td>Baden_Baden</td>\n",
       "      <td>48.765640</td>\n",
       "      <td>8.228524</td>\n",
       "      <td>-8.856667</td>\n",
       "      <td>-14.874167</td>\n",
       "      <td>-15.82</td>\n",
       "      <td>-3.96</td>\n",
       "      <td>1024.250000</td>\n",
       "      <td>84.208333</td>\n",
       "      <td>4.524583</td>\n",
       "      <td>100</td>\n",
       "      <td>Snow</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>5.7</td>\n",
       "      <td>1979</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1979-01-02</td>\n",
       "      <td>Bozanska</td>\n",
       "      <td>54.473028</td>\n",
       "      <td>18.302179</td>\n",
       "      <td>-9.037917</td>\n",
       "      <td>-15.179167</td>\n",
       "      <td>-14.14</td>\n",
       "      <td>-7.40</td>\n",
       "      <td>1004.791667</td>\n",
       "      <td>85.375000</td>\n",
       "      <td>4.449583</td>\n",
       "      <td>100</td>\n",
       "      <td>Clouds</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>7.6</td>\n",
       "      <td>1979</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        date    city_name        lat        lon       temp  \\\n",
       "0           0  1979-01-01  Baden_Baden  48.765640   8.228524 -10.442083   \n",
       "1           1  1979-01-01     Bozanska  54.473028  18.302179 -14.100000   \n",
       "2           2  1979-01-01  Walcz_Drugi  53.257039  16.521203 -16.437083   \n",
       "3           3  1979-01-02  Baden_Baden  48.765640   8.228524  -8.856667   \n",
       "4           4  1979-01-02     Bozanska  54.473028  18.302179  -9.037917   \n",
       "\n",
       "   feels_like  temp_min  temp_max     pressure   humidity  wind_speed  \\\n",
       "0  -16.532917    -16.18      3.02  1013.666667  81.916667    3.681667   \n",
       "1  -20.617500    -19.14     -9.46  1002.416667  82.000000    6.157917   \n",
       "2  -23.322500    -21.26     -9.57  1003.708333  82.416667    3.682083   \n",
       "3  -14.874167    -15.82     -3.96  1024.250000  84.208333    4.524583   \n",
       "4  -15.179167    -14.14     -7.40  1004.791667  85.375000    4.449583   \n",
       "\n",
       "   clouds_all weather_main  estimated_irradiance  temp_modifier  \\\n",
       "0         100         Snow                   1.5            1.0   \n",
       "1         100         Snow                   1.5            1.0   \n",
       "2         100       Clouds                   2.0            1.0   \n",
       "3         100         Snow                   1.5            1.0   \n",
       "4         100       Clouds                   2.0            1.0   \n",
       "\n",
       "   humidity_modifier  daily_energy_production_kWh  year  month  \n",
       "0               0.95                          5.7  1979      1  \n",
       "1               0.95                          5.7  1979      1  \n",
       "2               0.95                          7.6  1979      1  \n",
       "3               0.95                          5.7  1979      1  \n",
       "4               0.95                          7.6  1979      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49308 entries, 0 to 49307\n",
      "Data columns (total 20 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   Unnamed: 0                   49308 non-null  int64  \n",
      " 1   date                         49308 non-null  object \n",
      " 2   city_name                    49308 non-null  object \n",
      " 3   lat                          49308 non-null  float64\n",
      " 4   lon                          49308 non-null  float64\n",
      " 5   temp                         49308 non-null  float64\n",
      " 6   feels_like                   49308 non-null  float64\n",
      " 7   temp_min                     49308 non-null  float64\n",
      " 8   temp_max                     49308 non-null  float64\n",
      " 9   pressure                     49308 non-null  float64\n",
      " 10  humidity                     49308 non-null  float64\n",
      " 11  wind_speed                   49308 non-null  float64\n",
      " 12  clouds_all                   49308 non-null  int64  \n",
      " 13  weather_main                 49308 non-null  object \n",
      " 14  estimated_irradiance         49308 non-null  float64\n",
      " 15  temp_modifier                49308 non-null  float64\n",
      " 16  humidity_modifier            49308 non-null  float64\n",
      " 17  daily_energy_production_kWh  49308 non-null  float64\n",
      " 18  year                         49308 non-null  int64  \n",
      " 19  month                        49308 non-null  int64  \n",
      "dtypes: float64(13), int64(4), object(3)\n",
      "memory usage: 7.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(         Unnamed: 0           lat           lon          temp    feels_like  \\\n",
       " count  49308.000000  49308.000000  49308.000000  49308.000000  49308.000000   \n",
       " mean   24653.500000     52.165236     14.350635      8.896254      6.982085   \n",
       " std    14234.137873      2.454625      4.389665      7.747658      9.282311   \n",
       " min        0.000000     48.765640      8.228524    -22.734583    -29.682083   \n",
       " 25%    12326.750000     48.765640      8.228524      3.004271     -0.331528   \n",
       " 50%    24653.500000     53.257039     16.521203      9.069167      7.395208   \n",
       " 75%    36980.250000     54.473028     18.302179     15.077917     14.645938   \n",
       " max    49307.000000     54.473028     18.302179     30.162083     30.661667   \n",
       " \n",
       "            temp_min      temp_max      pressure      humidity    wind_speed  \\\n",
       " count  49308.000000  49308.000000  49308.000000  49308.000000  49308.000000   \n",
       " mean       4.408741     13.084188   1015.635358     80.171957      3.726556   \n",
       " std        7.069529      8.937748      9.251157     11.037107      1.755812   \n",
       " min      -30.120000    -19.150000    967.875000     33.125000      0.000000   \n",
       " 25%       -0.370000      5.890000   1010.166667     73.083333      2.425000   \n",
       " 50%        4.650000     13.240000   1015.916667     81.791667      3.430593   \n",
       " 75%        9.880000     19.950000   1021.458333     88.807692      4.705938   \n",
       " max       22.080000     38.990000   1049.708333    100.000000     15.084615   \n",
       " \n",
       "          clouds_all  estimated_irradiance  temp_modifier  humidity_modifier  \\\n",
       " count  49308.000000          49308.000000   49308.000000       49308.000000   \n",
       " mean      88.244504              2.511560       0.999714           0.972132   \n",
       " std       24.010742              1.295851       0.003770           0.024835   \n",
       " min        0.000000              1.000000       0.950000           0.950000   \n",
       " 25%       87.000000              2.000000       1.000000           0.950000   \n",
       " 50%      100.000000              2.000000       1.000000           0.950000   \n",
       " 75%      100.000000              2.000000       1.000000           1.000000   \n",
       " max      100.000000              5.000000       1.000000           1.000000   \n",
       " \n",
       "        daily_energy_production_kWh          year         month  \n",
       " count                 49308.000000  49308.000000  49308.000000  \n",
       " mean                      9.816561   2000.999331      6.522998  \n",
       " std                       5.204235     12.987107      3.448719  \n",
       " min                       3.800000   1979.000000      1.000000  \n",
       " 25%                       7.600000   1990.000000      4.000000  \n",
       " 50%                       7.600000   2001.000000      7.000000  \n",
       " 75%                       8.000000   2012.000000     10.000000  \n",
       " max                      20.000000   2023.000000     12.000000  ,\n",
       " None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Podstawowe informacje o danych\n",
    "data.describe(),data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oto podstawowe informacje o danych:**\n",
    "\n",
    "- ramka danych zawiera 49308 obserwacji i 20 kolumn.\n",
    "- Nie ma brakujących wartości w żadnej z kolumn, co jest dobrym znakiem dla analizy predykcyjnej.\n",
    "- Zmienne obejmują różnorodne cechy, takie jak temperatura (temp), ciśnienie (pressure), wilgotność (humidity), prędkość wiatru (wind_speed), a także wartości dotyczące produkcji energii (daily_energy_production_kWh).\n",
    "- Kolumna, którą chcemy przewidzieć to daily_energy_production_kWh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Analiza korelacji cech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzenie korelację poszczególnych cech z kolumną **daily_energy_production_kWh**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Obliczenie macierzy korelacji\n",
    "# correlation_matrix = data.corr()\n",
    "\n",
    "# # Wyświetlenie korelacji zmiennej celu z innymi zmiennymi\n",
    "# correlation_target = correlation_matrix['daily_energy_production_kWh'].sort_values(ascending=False)\n",
    "\n",
    "# # Wykres korelacji\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.barplot(y=correlation_target.index, x=correlation_target.values, hue=correlation_target.index, palette='coolwarm', legend=False)\n",
    "# plt.title('Korelacja z daily_energy_production_kWh')\n",
    "# plt.xlabel('Współczynnik korelacji')\n",
    "# plt.show()\n",
    "\n",
    "# correlation_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Assuming 'data' is a DataFrame already defined and loaded properly.\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'data' is a DataFrame already defined and loaded properly.\n",
    "correlation_matrix = data.corr()\n",
    "\n",
    "# Displaying the correlation of the target variable with other variables.\n",
    "correlation_target = correlation_matrix['daily_energy_production_kWh'].sort_values(ascending=False)\n",
    "\n",
    "# Plotting the correlation\n",
    "plt.figure(figsize=(10, 8))\n",
    "barplot = sns.barplot(y=correlation_target.index, x=correlation_target.values, palette='coolwarm')\n",
    "plt.title('Correlation with daily_energy_production_kWh')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "\n",
    "# If you decide you don't want a legend, you can disable it using plt.legend().\n",
    "plt.legend().set_visible(False)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print the sorted correlation values.\n",
    "correlation_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analiza korelacji ujawniła kilka interesujących zależności:\n",
    "\n",
    "- Zmienna estimated_irradiance ma bardzo wysoką korelację (prawie 1) z daily_energy_production_kWh, co wskazuje, że jest to kluczowy czynnik wpływający na produkcję energii.\n",
    "- Inne zmienne takie jak humidity_modifier, temp_max, temp, i feels_like również pokazują pozytywną korelację z produkcją energii, chociaż są znacznie słabsze.\n",
    "- Zmienne takie jak humidity i clouds_all mają silną ujemną korelację z produkcją energii, co oznacza, że wyższe wartości tych zmiennych mogą prowadzić do mniejszej produkcji energii."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Wybór modeli i cech do modelowania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na podstawie analizy korelacji, warto wybrać modele, które najlepiej radzą sobie z regresją, ponieważ mamy do czynienia z ciągłą zmienną celu (produkcja energii). Oto modele z sklearn, które weźmiemy pod uwagę:\n",
    "\n",
    "1. Regresja liniowa (dobra dla liniowych zależności)\n",
    "2. Drzewo decyzyjne (może modelować nieliniowe zależności)\n",
    "3. RandomForest (ensemble drzew decyzyjnych, bardziej rozbudowany i zazwyczaj bardziej dokładny)\n",
    "4. XGBoost (bardzo skuteczny model gradient boosting)\n",
    "\n",
    "**Uwaga:**\n",
    "Modele takie jak NearestCentroid, KNeighborsClassifier, Naive Bayes, i SVC są zazwyczaj stosowane do klasyfikacji, więc nie będą odpowiednie do naszego przypadku."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Przygotowanie danych do modelowania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wybiorę następujące cechy na podstawie korelacji:\n",
    "\n",
    "- estimated_irradiance\n",
    "- humidity_modifier\n",
    "- temp_max\n",
    "- temp\n",
    "- feels_like\n",
    "- pressure\n",
    "- humidity\n",
    "- clouds_all\n",
    "- year\n",
    "- month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as 'filefind' could not be imported from 'c:\\Users\\reszk\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\traitlets\\utils\\__init__.py'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Wybór cech na podstawie analizy korelacji\n",
    "features = ['estimated_irradiance', 'humidity_modifier', 'temp_max', 'temp', 'feels_like', 'pressure', 'humidity', 'clouds_all', 'year', 'month']\n",
    "target = 'daily_energy_production_kWh'\n",
    "\n",
    "# Przygotowanie danych\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Podział danych na zestaw treningowy i testowy (80% trening, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Weryfikacja rozmiarów zbiorów\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dane zostały podzielone na zbiory treningowe i testowe. Mamy 39446 obserwacji w zbiorze treningowym i 9862 w zbiorze testowym, przy wykorzystaniu 8 cech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Trenowanie modeli na ustawieniach domyślnych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaczniemy od trenowania czterech wybranych modeli:\n",
    "\n",
    "1. Regresja liniowa\n",
    "2. Drzewo decyzyjne\n",
    "3. Random Forest\n",
    "4. XGBoost\n",
    "\n",
    "**Regresja liniowa:**\n",
    "Regresja liniowa jest jednym z podstawowych algorytmów używanych w analizie regresji. Zakłada ona liniową zależność między zmiennymi niezależnymi (cechy) a zmienną zależną (wartością przewidywaną). Celem regresji liniowej jest znalezienie najlepszej dopasowanej linii (lub hiperpłaszczyzny w przypadku wielu zmiennych niezależnych), która minimalizuje błąd predykcji. Proces ten polega na znalezieniu zestawu parametrów (wag), które minimalizują sumę kwadratów różnic między wartościami przewidywanymi a rzeczywistymi. Regresja liniowa jest stosowana w przypadku, gdy zmienna zależna jest ciągła.\n",
    "\n",
    "**Drzewo decyzyjne:**\n",
    "Drzewo decyzyjne jest modelem predykcyjnym używanym zarówno w zadaniach klasyfikacji, jak i regresji. Model ten składa się z drzewa struktury podziałów decyzyjnych. W każdym węźle drzewa następuje podział danych na podstawie wartości jednej z cech, aby zminimalizować nieczystość węzła (np. entropię w przypadku klasyfikacji). Proces ten jest rekursywnie powtarzany, aż do osiągnięcia warunku stopu, np. maksymalnej głębokości drzewa lub minimalnej liczby próbek w liściach. W przypadku regresji, liście drzewa zawierają średnią wartość zmiennej zależnej dla odpowiednich grup danych.\n",
    "\n",
    "**Random Forest:**\n",
    "Random Forest jest algorytmem zbudowanym na bazie drzew decyzyjnych. Tworzy on wiele drzew decyzyjnych w procesie zwanym baggingiem (z ang. bootstrap aggregating), gdzie każde drzewo jest uczone na losowym podzbiorze danych treningowych. Następnie, przewidywanie w Random Forest jest wykonywane poprzez uśrednienie przewidywań wszystkich drzew decyzyjnych w przypadku klasyfikacji lub uśrednienie przewidywanych wartości w przypadku regresji. Random Forest jest odporny na overfitting i zwykle oferuje lepszą wydajność niż pojedyncze drzewo decyzyjne.\n",
    "\n",
    "**XGBoost:**\n",
    "XGBoost (Extreme Gradient Boosting) to technika boostingowa, która skupia się na sekwencyjnym trenowaniu wielu słabych modeli predykcyjnych, takich jak drzewa decyzyjne, w celu utworzenia silnego modelu. XGBoost wykorzystuje technikę gradient boosting, w której każdy kolejny model jest uczony w taki sposób, aby minimalizować błąd predykcji poprzednich modeli. XGBoost wprowadza dodatkowe innowacje, takie jak regularyzacja drzew, oszacowanie drugiego rzędu (dla bardziej stabilnych prognoz) i optymalizacja obliczeń, aby zapewnić wyższą wydajność i dokładność. Jest powszechnie stosowany w konkursach na platformie Kaggle oraz w różnych zastosowaniach przemysłowych, gdzie precyzja ma kluczowe znaczenie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ocena modelow nastapi przy użyciu metryk takich jak średni błąd kwadratowy (MSE) i średni błąd bezwzględny (MAE) na zbiorze testowym\n",
    "\n",
    "**Średni błąd kwadratowy (MSE):**\n",
    "MSE jest średnią z kwadratów różnic pomiędzy rzeczywistymi (obserwowanymi) wartościami a przewidywanymi wartościami. Jest to popularna metryka używana do oceny jakości regresji, ponieważ większe błędy są bardziej karane niż mniejsze błędy (poprzez kwadratowanie różnicy). MSE może przyjąć wartości od 0 do nieskończoności, gdzie niższa wartość wskazuje lepszą jakość predykcji. Oczekuje się, że niższa wartość MSE będzie lepsza.\n",
    "\n",
    "**Średni błąd bezwzględny (MAE):**\n",
    "MAE jest średnią z bezwzględnych różnic pomiędzy rzeczywistymi (obserwowanymi) wartościami a przewidywanymi wartościami. Jest to inna popularna metryka do oceny jakości regresji. MAE oblicza średnią bezwzględną odległość pomiędzy przewidywanymi a rzeczywistymi wartościami, co sprawia, że nie jest tak wrażliwa na duże błędy jak MSE. MAE również może przyjmować wartości od 0 do nieskończoności, gdzie niższa wartość wskazuje lepszą jakość predykcji. Oczekuje się, że niższa wartość MAE będzie lepsza.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as 'filefind' could not be imported from 'c:\\Users\\reszk\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\traitlets\\utils\\__init__.py'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Inicjalizacja modeli\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Trenowanie modeli i obliczanie błędów\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    results[name] = {\"MSE\": mse, \"MAE\": mae}\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obserwacje:**\n",
    "- Drzewo decyzyjne i XGBoost wykazały wyjątkowo niskie wartości błędów, sugerujące bardzo dobre dopasowanie modelu, może nawet zbyt dobre, co może wskazywać na nadmierne dopasowanie.\n",
    "\n",
    "- Random Forest także osiągnął bardzo dobre wyniki, lecz z nieco większymi błędami niż drzewo decyzyjne i XGBoost.\n",
    "\n",
    "- Regresja liniowa zapewniła najgorsze wyniki spośród modeli, ale nadal z akceptowalnymi błędami dla niektórych zastosowań."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Optymalizacja hiperparametrów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozpoczynamy proces optymalizacji hiperparametrów dla modeli Random Forest i XGBoost przy użyciu metody Grid Search. Definiujemy zestaw parametrów do przetestowania dla każdego modelu i przeprowadzę przeszukiwanie, aby znaleźć te, które minimalizują błąd predykcji na zbiorze testowym.\n",
    "\n",
    "**Dla Regresji Liniowej:**\n",
    "- Regresja liniowa zwykle nie wymaga optymalizacji hiperparametrów, ponieważ jest to bardzo prosty model i nie posiada hiperparametrów, które mogłyby być dostosowywane w sposób, jaki to robimy dla modeli opartych na drzewach.\n",
    "\n",
    "**Optymalizacja hiperparametrów dla Drzewa Decyzyjnego**\n",
    "- max_depth: ograniczenie głębokości drzewa może pomóc w zapobieganiu nadmiernemu dopasowaniu.\n",
    "- min_samples_split: minimalna liczba próbek potrzebna do podziału węzła; większa wartość może również pomóc w generalizacji modelu.\n",
    "\n",
    "**Optymalizacja hiperparametrów dla Random Forest**\n",
    "- n_estimators: liczba drzew w lesie\n",
    "- max_depth: maksymalna głębokość drzew\n",
    "- min_samples_split: minimalna liczba próbek wymagana do podziału wewnętrznego węzła\n",
    "\n",
    "**Optymalizacja hiperparametrów dla XGBoost**\n",
    "- n_estimators: liczba drzew\n",
    "- learning_rate: współczynnik uczenia się\n",
    "- max_depth: maksymalna głębokość każdego drzewa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as 'filefind' could not be imported from 'c:\\Users\\reszk\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\traitlets\\utils\\__init__.py'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Parametry do przeszukania dla Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Inicjalizacja GridSearchCV dla Random Forest\n",
    "grid_search_rf = GridSearchCV(estimator=RandomForestRegressor(random_state=42),\n",
    "                              param_grid=param_grid_rf,\n",
    "                              scoring='neg_mean_squared_error', # Używamy negatywnego MSE\n",
    "                              cv=3, # 3-krotna walidacja krzyżowa\n",
    "                              verbose=2,\n",
    "                              n_jobs=-1)  # Wykorzystanie wszystkich dostępnych rdzeni\n",
    "\n",
    "# Uruchomienie Grid Search dla Random Forest\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Najlepsze parametry i wynik\n",
    "best_params_rf = grid_search_rf.best_params_\n",
    "best_score_rf = grid_search_rf.best_score_\n",
    "\n",
    "best_params_rf, best_score_rf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as 'filefind' could not be imported from 'c:\\Users\\reszk\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\traitlets\\utils\\__init__.py'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Parametry do przeszukania dla XGBoost\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 150],\n",
    "    'learning_rate': [0.01, 0.05],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "\n",
    "# Inicjalizacja GridSearchCV dla XGBoost\n",
    "grid_search_xgb = GridSearchCV(estimator=XGBRegressor(random_state=42),\n",
    "                               param_grid=param_grid_xgb,\n",
    "                               scoring='neg_mean_squared_error',  # Używamy negatywnego MSE\n",
    "                               cv=3,  # 3-krotna walidacja krzyżowa\n",
    "                               verbose=2,\n",
    "                               n_jobs=-1)  # Wykorzystanie wszystkich dostępnych rdzeni\n",
    "\n",
    "# Uruchomienie Grid Search dla XGBoost\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Najlepsze parametry i wynik\n",
    "best_params_xgb = grid_search_xgb.best_params_\n",
    "best_score_xgb = grid_search_xgb.best_score_\n",
    "\n",
    "best_params_xgb, best_score_xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as 'filefind' could not be imported from 'c:\\Users\\reszk\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\traitlets\\utils\\__init__.py'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Parametry do przeszukania dla Drzewa Decyzyjnego\n",
    "param_grid_dt = {\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10, 20]\n",
    "}\n",
    "\n",
    "# Inicjalizacja GridSearchCV dla Drzewa Decyzyjnego\n",
    "grid_search_dt = GridSearchCV(estimator=DecisionTreeRegressor(random_state=42),\n",
    "                              param_grid=param_grid_dt,\n",
    "                              scoring='neg_mean_squared_error',  # Używamy negatywnego MSE\n",
    "                              cv=3,  # 3-krotna walidacja krzyżowa\n",
    "                              verbose=2,\n",
    "                              n_jobs=-1)  # Wykorzystanie wszystkich dostępnych rdzeni\n",
    "\n",
    "# Uruchomienie Grid Search dla Drzewa Decyzyjnego\n",
    "grid_search_dt.fit(X_train, y_train)\n",
    "\n",
    "# Najlepsze parametry i wynik\n",
    "best_params_dt = grid_search_dt.best_params_\n",
    "best_score_dt = grid_search_dt.best_score_\n",
    "\n",
    "best_params_dt, best_score_dt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wytrenowanie modeli z optymalnymi parametrami**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Drzewo decyzyjne** z max_depth=None i min_samples_split=2.\n",
    "\n",
    "- **Random Forest** z max_depth=10, min_samples_split=2, i n_estimators=50.\n",
    "\n",
    "- **XGBoost** z learning_rate=0.05, max_depth=5, i n_estimators=150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as 'filefind' could not be imported from 'c:\\Users\\reszk\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\traitlets\\utils\\__init__.py'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Re-trenowanie modeli z zoptymalizowanymi parametrami\n",
    "optimized_models = {\n",
    "    \"Optimized Decision Tree\": DecisionTreeRegressor(max_depth=None, min_samples_split=2, random_state=42),\n",
    "    \"Optimized Random Forest\": RandomForestRegressor(n_estimators=50, max_depth=10, min_samples_split=2, random_state=42),\n",
    "    \"Optimized XGBoost\": XGBRegressor(n_estimators=150, max_depth=5, learning_rate=0.05, random_state=42)\n",
    "    \n",
    "}\n",
    "\n",
    "# Ocena modeli\n",
    "optimized_results = {}\n",
    "for name, model in optimized_models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    optimized_results[name] = {\"MSE\": mse, \"MAE\": mae}\n",
    "\n",
    "optimized_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as 'filefind' could not be imported from 'c:\\Users\\reszk\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\traitlets\\utils\\__init__.py'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "optimized_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as 'filefind' could not be imported from 'c:\\Users\\reszk\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\traitlets\\utils\\__init__.py'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Tworzenie DataFrame do prezentacji wyników\n",
    "df_results = pd.DataFrame.from_dict(results, orient='index')\n",
    "df_optimized_results = pd.DataFrame.from_dict(optimized_results, orient='index')\n",
    "\n",
    "# Przygotowanie DataFrame z wynikami\n",
    "df_results = pd.DataFrame.from_dict(results, orient='index')\n",
    "df_optimized_results = pd.DataFrame.from_dict(optimized_results, orient='index')\n",
    "\n",
    "# Konwersja DataFrame, aby kolumny były wierszami\n",
    "df_results = df_results.T\n",
    "df_optimized_results = df_optimized_results.T\n",
    "\n",
    "# Połączenie wyników w jedną tabelę, konfigurując indeksy wierszy\n",
    "comparison_table = pd.concat([df_results, df_optimized_results], axis=1)\n",
    "\n",
    "# Wyświetlenie tabeli porównawczej\n",
    "comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Normalizacja danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Będziemy stosować normalizację z-score, co polega na odjęciu średniej i podzieleniu przez odchylenie standardowe każdej cechy. Normalizacja ta zapewnia, że każda cecha ma średnią równą 0 i odchylenie standardowe równe 1, co jest szczególnie pomocne w modelowaniu statystycznym i uczeniu maszynowym.\n",
    "\n",
    "1. Normalizacja cech\n",
    "\n",
    "2. Trenowanie modeli na znormalizowanych danych\n",
    "\n",
    "3. Porównanie wyników z danymi nieznormalizowanymi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as 'filefind' could not be imported from 'c:\\Users\\reszk\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\traitlets\\utils\\__init__.py'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Inicjalizacja scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Normalizacja danych treningowych i testowych\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Trenowanie modeli na znormalizowanych danych\n",
    "models_scaled = {\n",
    "    \"Linear Regression_scaled\": LinearRegression(),\n",
    "    \"Decision Tree_scaled\": DecisionTreeRegressor(max_depth=None, min_samples_split=2, random_state=42),\n",
    "    \"Random Forest_scaled\": RandomForestRegressor(n_estimators=50, max_depth=10, min_samples_split=2, random_state=42),\n",
    "    \"XGBoost_scaled\": XGBRegressor(n_estimators=150, max_depth=5, learning_rate=0.05, random_state=42)\n",
    "}\n",
    "\n",
    "# Wyniki modeli na znormalizowanych danych\n",
    "results_scaled = {}\n",
    "for name, model in models_scaled.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred_scaled = model.predict(X_test_scaled)\n",
    "    mse_scaled = mean_squared_error(y_test, y_pred_scaled)\n",
    "    mae_scaled = mean_absolute_error(y_test, y_pred_scaled)\n",
    "    results_scaled[name] = {\"MSE\": mse_scaled, \"MAE\": mae_scaled}\n",
    "\n",
    "results_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as 'filefind' could not be imported from 'c:\\Users\\reszk\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\traitlets\\utils\\__init__.py'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Przygotowanie DataFrame z wynikami\n",
    "df_results = pd.DataFrame.from_dict(results, orient='index')\n",
    "df_optimized_results = pd.DataFrame.from_dict(optimized_results, orient='index')\n",
    "df_results_scaled = pd.DataFrame.from_dict(results_scaled, orient='index')\n",
    "\n",
    "# Konwersja DataFrame, aby kolumny były wierszami\n",
    "df_results = df_results.T\n",
    "df_optimized_results = df_optimized_results.T\n",
    "df_results_scaled = df_results_scaled.T\n",
    "\n",
    "# Połączenie wyników w jedną tabelę\n",
    "comparison_table = pd.concat([df_results, df_optimized_results, df_results_scaled], axis=1)\n",
    "\n",
    "# Wyświetlenie tabeli porównawczej\n",
    "comparison_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wnisoek:**\n",
    "- najelpszy model to: Optimized Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as 'filefind' could not be imported from 'c:\\Users\\reszk\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\traitlets\\utils\\__init__.py'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Wybieramy najlepszy model\n",
    "best_model = optimized_models[\"Optimized Random Forest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as 'filefind' could not be imported from 'c:\\Users\\reszk\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\traitlets\\utils\\__init__.py'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Ścieżka do zapisania modelu\n",
    "model_path = '../2. Data/ML_models/optimal_random_forest_model.joblib'\n",
    "\n",
    "# Zapisanie modelu do pliku\n",
    "joblib.dump(best_model, model_path)\n",
    "\n",
    "print(\"Model został zapisany do:\", model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Ekstrapolacja w przyszłość"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby przeprowadzić predykcje na przyszłe lata, będziesz potrzebował przyszłych danych, które mają te same cechy, co dane używane do trenowania modelu. To może wymagać estymacji lub prognozowania niektórych wejść, takich jak pogoda lub inne zmienne, które wpływają na produkcję energii. Oto, co możemy zrobić:\n",
    "\n",
    "1.  Przygotowanie danych przyszłych: Jeżeli nie masz rzeczywistych danych przyszłych, możesz użyć modeli prognozowania dla zmiennych wejściowych lub użyć danych historycznych jako proxy dla przyszłych trendów.\n",
    "\n",
    "2.  Predykcja z użyciem modelu: Używając przygotowanych danych przyszłych, wykonaj predykcje za pomocą zapisanego modelu.\n",
    "\n",
    "3.  Analiza wyników: Ocen, jakie są wyniki predykcji i jakie mogą być ich implikacje dla planowania i zarządzania produkcją energii."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeżeli masz tylko dane historyczne, możesz podejść do tego w kilka sposobów, aby zrealizować predykcje na lata 2024-2035. Oto kilka możliwych opcji:\n",
    "\n",
    "1. Użyj średnich wartości historycznych\n",
    "Możesz wykorzystać średnie wartości cech z danych historycznych jako aproksymację przyszłych warunków. To może być dobre rozwiązanie, jeżeli nie spodziewasz się dużych zmian w warunkach, które miały wpływ na produkcję energii (np. klimat, technologia).\n",
    "\n",
    "2. Ekstrapolacja trendów\n",
    "Jeśli masz powody sądzić, że pewne trendy w danych będą kontynuowane, możesz próbować ekstrapolować te trendy na przyszłe lata. Na przykład, jeśli temperatura wzrastała w ostatnich latach, możesz zamodelować ten trend i przewidzieć przyszłe temperatury.\n",
    "\n",
    "3. Stworzenie syntetycznych danych\n",
    "Możesz użyć technik takich jak symulacje Monte Carlo do generowania możliwych przyszłych scenariuszy danych na podstawie rozkładów i korelacji obserwowanych w danych historycznych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nie rozumiem to po co trenowalismy nasz model , skoro teraz musimy przewidywac wszystkie cechy zeby sprawdzic przewidywania dla produkcji energi.\n",
    "\n",
    "do trenowania naszego modelu wybralem zgodnie z podanym kodem:\n",
    "Wybiorę następujące cechy na podstawie korelacji:\n",
    "\n",
    "- estimated_irradiance\n",
    "- humidity_modifier\n",
    "- temp_max\n",
    "- temp\n",
    "- feels_like\n",
    "- pressure\n",
    "- humidity\n",
    "- clouds_all\n",
    "\n",
    "to nie lepiej bylo by przewidywac kazada zmienna z osobna i pozniej zbudowac dataFrame gdzie jest wyliczone nasza produkcja energi? moze nie dalo by sie wpisywac wartosci jak juz dane beda dostepne, i uzyc modelu wtedy ktorego nie mamy,  czy mam racje? Napisz tylko nic nie licz.\n",
    "\n",
    "czy w nastepnym kroku mozemy wgrac nasz zapisany model, wygenerowac przyszle dane na podstawie danych historycznych ktorych uzylismy do naszego modelu zeby pozniej przewidziec produkcje energii dla przyszlych lat.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masz rację, że przewidywanie każdej zmiennej osobno, aby zbudować przyszły zestaw danych, który wykorzystasz do przewidzenia produkcji energii, jest jedną z metod, ale może być dość skomplikowana i czasochłonna. Każda z tych zmiennych mogłaby wymagać osobnego modelu predykcyjnego, co znacznie komplikuje cały proces.\n",
    "\n",
    "Zwykle, gdy modelujemy produkcję energii (lub inną zmienną zależną) na podstawie różnych cech, zakładamy, że przyszłe wartości tych cech będą dostępne albo poprzez bezpośrednie pomiary, albo przez modele, które mogą je przewidzieć (np. modele pogodowe). W przypadku braku takich modeli czy danych, zwykle korzysta się z uproszczonych podejść, takich jak ekstrapolacja statystyczna (np. średnie historyczne), aby wstępnie ocenić przyszłe wartości.\n",
    "\n",
    "Proponowane podejście\n",
    "Ponieważ trenowałeś już model predykcyjny dla produkcji energii, używając specyficznych cech, rozsądne jest wykorzystanie dostępnych danych historycznych do stworzenia prostych ekstrapolacji tych cech w przyszłość. Na przykład:\n",
    "\n",
    "Średnie wartości: Możesz przyjąć, że przyszłe wartości cech będą równać się ich średnim wartościom z danych historycznych lub z ostatnich kilku lat, jeśli spodziewane są pewne trendowe zmiany.\n",
    "\n",
    "Trendy: Jeśli dane historyczne pokazują wyraźne trendy (np. wzrost temperatury), można użyć prostego modelowania trendów, jak sugerowałem wcześniej.\n",
    "\n",
    "Użycie modelu do predykcji produkcji energii: Gdy masz już przyszłe dane dla cech, możesz załadować swój wytrenowany model i użyć go do przewidywania produkcji energii na podstawie tych cech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as 'filefind' could not be imported from 'c:\\Users\\reszk\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\traitlets\\utils\\__init__.py'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Załaduj zapisany model\n",
    "model_path = '../2. Data/ML_models/optimal_random_forest_model.joblib'\n",
    "best_model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozumiem twoje wątpliwości. Proces, który opisałem, dotyczył tylko ekstrapolacji trendów dla jednej lub kilku zmiennych, które mogą być kluczowe w przypadku braku dostępnych przyszłych danych dla tych zmiennych. Takie podejście jest często stosowane, gdy przewidywania oparte na modelu wymagają danych wejściowych, które również muszą być prognozowane.\n",
    "\n",
    "Dlaczego trenowaliśmy model?\n",
    "Model, który trenowaliśmy, służy do przewidywania produkcji energii na podstawie różnych parametrów meteorologicznych i innych czynników wpływających na produkcję. Prognozowanie tych zmiennych wejściowych jest potrzebne tylko wtedy, gdy nie mamy ich przyszłych wartości bezpośrednio, ale znamy ich historyczne trendy lub możemy oszacować ich przyszłe zmiany.\n",
    "\n",
    "Co można zrobić, jeśli mamy tylko dane historyczne?\n",
    "Użyj średnich wartości historycznych: Jeśli uważasz, że zmienne nie ulegną znaczącej zmianie, możesz użyć średnich wartości z danych historycznych dla każdej cechy. To prosty, ale czasem skuteczny sposób na przygotowanie danych wejściowych dla modelu predykcyjnego.\n",
    "\n",
    "Ekstrapolacja trendów: Dla zmiennych, które mogą mieć ważny trend (np. temp, pressure), możesz modelować ich przyszłe wartości używając metod takich jak regresja liniowa, jak opisano wcześniej. Dla innych zmiennych, które są mniej podatne na długoterminowe trendy, możesz zastosować inne metody statystyczne lub ich średnie wartości.\n",
    "\n",
    "Symulacja Monte Carlo lub inne metody statystyczne: Możesz także zastosować bardziej zaawansowane metody generowania przyszłych scenariuszy danych na podstawie rozkładów prawdopodobieństwa zmiennych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby przeprowadzić kompleksową analizę i predykcję, postępując krok po kroku, zaczniemy od przygotowania danych historycznych na przyszłe lata, uwzględniając średnie wartości oraz ewentualne trendy. Następnie wykorzystamy najlepszy model do predykcji produkcji energii na lata 2024-2035. Poniżej przedstawię sposób postępowania:\n",
    "\n",
    "1. Średnie wartości\n",
    "Utworzymy ramkę danych na przyszłe lata, wykorzystując średnie wartości cech z danych historycznych. To pozwoli na proste oszacowanie przyszłych danych, na podstawie których nasz model przewidzi produkcję energii.\n",
    "\n",
    "2. Trendy\n",
    "Jeśli w danych historycznych istnieją wyraźne trendy, można je wykorzystać do lepszego oszacowania przyszłych wartości. Przykładowo, jeśli temperatura ma trend wzrostowy, możemy dostosować średnią temperaturę na przyszłe lata, biorąc pod uwagę ten trend.\n",
    "\n",
    "3. Predykcja\n",
    "Wykorzystamy wytrenowany model Optimized Random Forest do predykcji produkcji energii na podstawie przyszłych danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To podejście pozwala na wykorzystanie dostępnych danych historycznych do przygotowania przyszłych danych, które są następnie używane do predykcji za pomocą najlepszego modelu. Jest to efektywne i stosunkowo proste rozwiązanie, gdy nie dysponuje się skomplikowanymi modelami do prognozowania każdej cechy osobno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as 'filefind' could not be imported from 'c:\\Users\\reszk\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\traitlets\\utils\\__init__.py'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Wczytanie danych z pliku CSV\n",
    "data_path = '../2. Data/processed/final_df_processed.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Wybór cech na podstawie analizy korelacji\n",
    "features = ['estimated_irradiance', 'humidity_modifier', 'temp_max', 'temp', 'feels_like', 'pressure', 'humidity', 'clouds_all']\n",
    "target = 'daily_energy_production_kWh'\n",
    "\n",
    "# Przygotowanie danych\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Podział danych na zestaw treningowy i testowy (80% trening, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Średnie wartości cech\n",
    "future_data = X_train.mean().to_frame().T  # Przygotowanie danych przyszłych\n",
    "\n",
    "# Załaduj zapisany model\n",
    "model_path = '../2. Data/ML_models/optimal_random_forest_model.joblib'\n",
    "best_model = joblib.load(model_path)\n",
    "\n",
    "# Użyj modelu do przewidywania produkcji energii\n",
    "predicted_energy = best_model.predict(future_data)\n",
    "print(\"Przewidywana produkcja energii na rok 2024-2035:\", predicted_energy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wydaje się, że podczas trenowania modelu nie uwzględniłeś kolumny \"year\". W takim przypadku, aby dokonać prognozy na podstawie nowej kolumny \"year\", musisz zmodyfikować sposób, w jaki trenujesz model. Oto jak możesz to zrobić:\n",
    "\n",
    "Uwzględnij kolumnę \"year\" podczas trenowania modelu: Jeśli \"year\" jest ważnym czynnikiem dla prognoz, dodaj go jako cechę podczas trenowania modelu.\n",
    "\n",
    "Usunięcie kolumny \"year\" z danych przyszłych: Jeśli kolumna \"year\" nie jest używana do prognozowania, usuń ją z danych przyszłych przed przewidywaniem.\n",
    "\n",
    "\n",
    "**moze trzeba year, month, location name?zeby pozniej bylo dla danej lokalizacji**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as 'filefind' could not be imported from 'c:\\Users\\reszk\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\traitlets\\utils\\__init__.py'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Wczytanie danych z pliku CSV\n",
    "data_path = '../2. Data/processed/final_df_processed.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Wybór cech na podstawie analizy korelacji\n",
    "features = ['estimated_irradiance', 'humidity_modifier', 'temp_max', 'temp', 'feels_like', 'pressure', 'humidity', 'clouds_all']\n",
    "target = 'daily_energy_production_kWh'\n",
    "\n",
    "# Przygotowanie danych\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Podział danych na zestaw treningowy i testowy (80% trening, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Załaduj zapisany model\n",
    "model_path = '../2. Data/ML_models/optimal_random_forest_model.joblib'\n",
    "best_model = joblib.load(model_path)\n",
    "\n",
    "# Przygotowanie danych przyszłych dla każdego roku z przedziału 2024-2035\n",
    "future_years = range(2024, 2036)\n",
    "predicted_energies = []\n",
    "\n",
    "for year in future_years:\n",
    "    future_data_copy = future_data.copy()  # Tworzenie kopii danych przyszłych\n",
    "    # future_data_copy['year'] = year  # Jeśli 'year' nie jest cechą modelu, usuń tę linię\n",
    "    predicted_energy = best_model.predict(future_data_copy)\n",
    "    predicted_energies.append(predicted_energy[0])  # Dodanie przewidywanej energii dla danego roku\n",
    "\n",
    "# Wydrukowanie przewidywanej produkcji energii dla każdego roku\n",
    "for year, energy in zip(future_years, predicted_energies):\n",
    "    print(f\"Przewidywana produkcja energii na rok {year}: {energy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as 'filefind' could not be imported from 'c:\\Users\\reszk\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\traitlets\\utils\\__init__.py'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
